{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(1)\n",
    "COVID_label = {\"CT_NonCOVID\": 0, \"CT_COVID\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedir(new_dir):\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CT_COVID', '.DS_Store', 'CT_NonCOVID']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join(\"./data\")\n",
    "split_dir = os.path.join(\"split\")\n",
    "train_dir = os.path.join(split_dir, \"train\")\n",
    "valid_dir = os.path.join(split_dir, \"valid\")\n",
    "test_dir = os.path.join(split_dir, \"test\")\n",
    "print(os.listdir(dataset_dir))\n",
    "train_pct = 0.8\n",
    "valid_pct = 0.1\n",
    "test_pct = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 349/349 [00:00<00:00, 2489.76it/s]\n",
      "  0%|          | 0/397 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:CT_COVID, train:279, valid:35, test:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [00:00<00:00, 2423.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:CT_NonCOVID, train:317, valid:40, test:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for sub_dir in dirs:\n",
    "        imgs = os.listdir(os.path.join(root, sub_dir))\n",
    "        random.shuffle(imgs)\n",
    "        img_count = len(imgs)\n",
    "        \n",
    "        train_point = int(img_count * train_pct)\n",
    "        valid_point = int(img_count * (train_pct + valid_pct))\n",
    "        \n",
    "        for i in tqdm(range(img_count)):\n",
    "            if i < train_point:\n",
    "                out_dir = os.path.join(train_dir, sub_dir)\n",
    "            elif i < valid_point:\n",
    "                out_dir = os.path.join(valid_dir, sub_dir)\n",
    "            else:\n",
    "                out_dir = os.path.join(test_dir, sub_dir)\n",
    "\n",
    "            makedir(out_dir)\n",
    "\n",
    "            target_path = os.path.join(out_dir, imgs[i])\n",
    "            src_path = os.path.join(dataset_dir, sub_dir, imgs[i])\n",
    "\n",
    "            shutil.copy(src_path, target_path)\n",
    "\n",
    "        print('Class:{}, train:{}, valid:{}, test:{}'.format(sub_dir, train_point, valid_point-train_point,\n",
    "                                                                 img_count-valid_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    " \n",
    "        self.label_name = {\"CT_NonCOVID\": 0, \"CT_COVID\": 1}\n",
    "        self.data_info = self.get_img_info(data_dir)  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_img, label = self.data_info[index]\n",
    "        img = Image.open(path_img).convert('RGB')    \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   \n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "\n",
    "            for sub_dir in dirs:\n",
    "                img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                #img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "\n",
    "\n",
    "                for i in range(len(img_names)):\n",
    "                    img_name = img_names[i]\n",
    "                    path_img = os.path.join(root, sub_dir, img_name)\n",
    "                    label = COVID_label[sub_dir]\n",
    "                    data_info.append((path_img, int(label)))\n",
    "\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, 0, 0.1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class LeNet2(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet2, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.003\n",
    "log_interval = 10\n",
    "val_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = COVIDDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = COVIDDataset(data_dir=valid_dir, transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet(classes=2)\n",
    "net.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)                        \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_curve = list()\n",
    "valid_curve = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [16 x 44944], m2: [400 x 120] at ../aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-13ed3ef56e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fc8d1839afdf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [16 x 44944], m2: [400 x 120] at ../aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  \n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            valid_curve.append(loss.item())\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val, correct / total))\n",
    "\n",
    "\n",
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_loader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval \n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "net = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:Epoch[000/010] Iteration[010/046] Loss: 9.0230 Acc:0.62%\n",
      "Training:Epoch[000/010] Iteration[020/046] Loss: 9.1050 Acc:0.31%\n",
      "Training:Epoch[000/010] Iteration[030/046] Loss: 9.1678 Acc:0.21%\n",
      "Training:Epoch[000/010] Iteration[040/046] Loss: 8.9347 Acc:0.16%\n",
      "Valid:\t Epoch[000/010] Iteration[010/010] Loss: 105.5518 Acc:0.14%\n",
      "Training:Epoch[001/010] Iteration[010/046] Loss: 9.0799 Acc:0.00%\n",
      "Training:Epoch[001/010] Iteration[020/046] Loss: 9.1213 Acc:0.00%\n",
      "Training:Epoch[001/010] Iteration[030/046] Loss: 8.9698 Acc:0.00%\n",
      "Training:Epoch[001/010] Iteration[040/046] Loss: 8.9617 Acc:0.16%\n",
      "Valid:\t Epoch[001/010] Iteration[010/010] Loss: 101.0981 Acc:0.14%\n",
      "Training:Epoch[002/010] Iteration[010/046] Loss: 9.0035 Acc:0.00%\n",
      "Training:Epoch[002/010] Iteration[020/046] Loss: 9.1511 Acc:0.00%\n",
      "Training:Epoch[002/010] Iteration[030/046] Loss: 9.0336 Acc:0.00%\n",
      "Training:Epoch[002/010] Iteration[040/046] Loss: 9.0781 Acc:0.16%\n",
      "Valid:\t Epoch[002/010] Iteration[010/010] Loss: 101.9819 Acc:0.14%\n",
      "Training:Epoch[003/010] Iteration[010/046] Loss: 8.9674 Acc:0.00%\n",
      "Training:Epoch[003/010] Iteration[020/046] Loss: 8.9747 Acc:0.00%\n",
      "Training:Epoch[003/010] Iteration[030/046] Loss: 9.0020 Acc:0.00%\n",
      "Training:Epoch[003/010] Iteration[040/046] Loss: 9.0944 Acc:0.00%\n",
      "Valid:\t Epoch[003/010] Iteration[010/010] Loss: 101.0642 Acc:0.00%\n",
      "Training:Epoch[004/010] Iteration[010/046] Loss: 9.1090 Acc:0.00%\n",
      "Training:Epoch[004/010] Iteration[020/046] Loss: 9.0462 Acc:0.00%\n",
      "Training:Epoch[004/010] Iteration[030/046] Loss: 9.0381 Acc:0.00%\n",
      "Training:Epoch[004/010] Iteration[040/046] Loss: 9.0952 Acc:0.00%\n",
      "Valid:\t Epoch[004/010] Iteration[010/010] Loss: 100.8920 Acc:0.00%\n",
      "Training:Epoch[005/010] Iteration[010/046] Loss: 8.9866 Acc:0.00%\n",
      "Training:Epoch[005/010] Iteration[020/046] Loss: 9.0876 Acc:0.00%\n",
      "Training:Epoch[005/010] Iteration[030/046] Loss: 9.0451 Acc:0.00%\n",
      "Training:Epoch[005/010] Iteration[040/046] Loss: 8.9873 Acc:0.00%\n",
      "Valid:\t Epoch[005/010] Iteration[010/010] Loss: 102.1525 Acc:0.14%\n",
      "Training:Epoch[006/010] Iteration[010/046] Loss: 8.8992 Acc:0.00%\n",
      "Training:Epoch[006/010] Iteration[020/046] Loss: 9.0323 Acc:0.00%\n",
      "Training:Epoch[006/010] Iteration[030/046] Loss: 9.1835 Acc:0.00%\n",
      "Training:Epoch[006/010] Iteration[040/046] Loss: 9.0434 Acc:0.00%\n",
      "Valid:\t Epoch[006/010] Iteration[010/010] Loss: 101.7840 Acc:0.00%\n",
      "Training:Epoch[007/010] Iteration[010/046] Loss: 9.1571 Acc:0.00%\n",
      "Training:Epoch[007/010] Iteration[020/046] Loss: 9.1284 Acc:0.00%\n",
      "Training:Epoch[007/010] Iteration[030/046] Loss: 9.0009 Acc:0.00%\n",
      "Training:Epoch[007/010] Iteration[040/046] Loss: 8.8461 Acc:0.00%\n",
      "Valid:\t Epoch[007/010] Iteration[010/010] Loss: 102.5046 Acc:0.00%\n",
      "Training:Epoch[008/010] Iteration[010/046] Loss: 9.1583 Acc:0.00%\n",
      "Training:Epoch[008/010] Iteration[020/046] Loss: 9.0770 Acc:0.00%\n",
      "Training:Epoch[008/010] Iteration[030/046] Loss: 9.0300 Acc:0.21%\n",
      "Training:Epoch[008/010] Iteration[040/046] Loss: 9.0111 Acc:0.16%\n",
      "Valid:\t Epoch[008/010] Iteration[010/010] Loss: 100.2624 Acc:0.14%\n",
      "Training:Epoch[009/010] Iteration[010/046] Loss: 9.0307 Acc:0.00%\n",
      "Training:Epoch[009/010] Iteration[020/046] Loss: 9.1091 Acc:0.00%\n",
      "Training:Epoch[009/010] Iteration[030/046] Loss: 8.9810 Acc:0.21%\n",
      "Training:Epoch[009/010] Iteration[040/046] Loss: 9.1073 Acc:0.16%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  \n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            valid_curve.append(loss.item())\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val, correct / total))\n",
    "\n",
    "\n",
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_loader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval \n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
